{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Experiments and Final Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    " \"kernelspec\": {\n",
    "  \"display_name\": \"Python 3\",\n",
    "  \"language\": \"python\",\n",
    "  \"name\": \"python3\"\n",
    " },\n",
    " \"language_info\": {\n",
    "  \"codemirror_mode\": {\n",
    "   \"name\": \"ipython\",\n",
    "   \"version\": 3\n",
    "  },\n",
    "  \"file_extension\": \".py\",\n",
    "  \"mimetype\": \"text/x-python\",\n",
    "  \"name\": \"python\",\n",
    "  \"nbconvert_exporter\": \"python\",\n",
    "  \"pygments_lexer\": \"ipython3\",\n",
    "  \"version\": \"3.8.0\"\n",
    " }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.model_selection import cross_validate, train_test_split, GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Connect to the database and load the dataset\n",
    "conn = sqlite3.connect('mobile_phones.db')\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    p.phone_id,\n",
    "    p.battery_power,\n",
    "    p.clock_speed,\n",
    "    p.m_dep,\n",
    "    p.mobile_wt,\n",
    "    p.n_cores,\n",
    "    p.ram,\n",
    "    p.talk_time,\n",
    "    p.price_range,\n",
    "    s.px_height,\n",
    "    s.px_width,\n",
    "    s.sc_h,\n",
    "    s.sc_w,\n",
    "    c.fc as front_camera,\n",
    "    c.pc as primary_camera,\n",
    "    f.blue,\n",
    "    f.dual_sim,\n",
    "    f.four_g,\n",
    "    f.touch_screen,\n",
    "    f.wifi,\n",
    "    st.int_memory\n",
    "FROM phones p\n",
    "JOIN screen_specs s ON p.phone_id = s.phone_id\n",
    "JOIN camera_specs c ON p.phone_id = c.phone_id\n",
    "JOIN phone_features f ON p.phone_id = f.phone_id\n",
    "JOIN storage_specs st ON p.phone_id = st.phone_id\n",
    "\"\"\"\n",
    "df_db = pd.read_sql_query(query, conn)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing pipeline\n",
    "numerical_features = [\n",
    "    'battery_power', 'clock_speed', 'm_dep', 'mobile_wt',\n",
    "    'n_cores', 'ram', 'talk_time', 'sc_h', 'sc_w', 'int_memory'\n",
    "]\n",
    "categorical_features = ['blue', 'dual_sim', 'four_g', 'touch_screen', 'wifi']\n",
    "\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('minmax', MinMaxScaler()),\n",
    "    ('log', FunctionTransformer(np.log1p))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numerical_pipeline, numerical_features),\n",
    "    ('cat', OneHotEncoder(), categorical_features)\n",
    "])\n",
    "\n",
    "# Define scoring metrics\n",
    "scoring = {\n",
    "    'f1_macro': f1_score,\n",
    "    'precision_macro': precision_score,\n",
    "    'recall_macro': recall_score,\n",
    "    'accuracy': accuracy_score\n",
    "}\n",
    "\n",
    "# Split dataset\n",
    "X = df_db.drop(columns=['price_range', 'phone_id'])\n",
    "y = df_db['price_range']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize experiment tracking\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1: Logistic Regression with preprocessing and hyperparameter tuning\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(max_iter=2000, random_state=42))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'classifier__C': [0.1, 1, 10],\n",
    "    'classifier__solver': ['liblinear', 'lbfgs']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='f1_macro')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "results.append({'Experiment': 'Experiment 1', 'Model': 'Logistic Regression', 'F1': f1, 'Accuracy': accuracy, 'Overfitting': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "print(accuracy)\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the results as a table\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=2000, random_state=42),\n",
    "    'Ridge Classifier': RidgeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'XGBoost': XGBClassifier(random_state=42, eval_metric='mlogloss')\n",
    "}\n",
    "\n",
    "# Compare models\n",
    "for name, model in models.items():\n",
    "    try:\n",
    "        pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),  # Ensure preprocessor is defined\n",
    "            ('classifier', model)\n",
    "        ])\n",
    "\n",
    "        # Perform cross-validation\n",
    "        cv_results = cross_validate(\n",
    "            pipeline, X_train, y_train, cv=10, scoring='f1_macro', return_train_score=True\n",
    "        )\n",
    "\n",
    "        # Fit and predict\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "\n",
    "        # Calculate metrics\n",
    "        f1 = f1_score(y_test, y_pred, average='macro')\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        # Append results\n",
    "        results.append({\n",
    "            'Experiment': 'Experiment 2',\n",
    "            'Model': name,\n",
    "            'F1': f1,\n",
    "            'Accuracy': accuracy,\n",
    "        })\n",
    "    except Exception:\n",
    "        print(f\"Failed to run {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Experiment 3: Feature engineering\n",
    "X_train['total_pixels'] = X_train['px_height'] * X_train['px_width']\n",
    "X_test['total_pixels'] = X_test['px_height'] * X_test['px_width']\n",
    "\n",
    "X_train['performance'] = X_train['battery_power'] * X_train['ram']\n",
    "X_test['performance'] = X_test['battery_power'] * X_test['ram']\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "results.append({'Experiment': 'Experiment 3', 'Model': 'Random Forest', 'F1': f1, 'Accuracy': accuracy, 'Overfitting': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "print(accuracy)\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the results as a table\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 4: Feature Selection\n",
    "\n",
    "# First apply preprocessing to get consistent features\n",
    "preprocessed_train = preprocessor.fit_transform(X_train)\n",
    "preprocessed_test = preprocessor.transform(X_test)\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "feature_names = (\n",
    "    numerical_features +\n",
    "    [f\"{feat}_{val}\" for feat, vals in \n",
    "     zip(categorical_features, \n",
    "         preprocessor.named_transformers_['cat'].categories_) \n",
    "     for val in vals]\n",
    ")\n",
    "\n",
    "# Convert to DataFrame with proper feature names - remove toarray()\n",
    "X_train_processed = pd.DataFrame(\n",
    "    preprocessed_train,\n",
    "    columns=feature_names,\n",
    "    index=X_train.index\n",
    ")\n",
    "X_test_processed = pd.DataFrame(\n",
    "    preprocessed_test,\n",
    "    columns=feature_names,\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "# Now apply Variance Threshold\n",
    "variance_thresholder = VarianceThreshold(threshold=0.1)\n",
    "X_train_var = variance_thresholder.fit_transform(X_train_processed)\n",
    "X_test_var = variance_thresholder.transform(X_test_processed)\n",
    "\n",
    "# Get selected feature names\n",
    "selected_features = X_train_processed.columns[variance_thresholder.get_support()]\n",
    "\n",
    "# Create pipeline for the classifier only (preprocessing already done)\n",
    "pipeline = Pipeline([\n",
    "    ('classifier', LogisticRegression(max_iter=2000, random_state=42))\n",
    "])\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_results = cross_validate(\n",
    "    pipeline, X_train_var, y_train,\n",
    "    cv=10, scoring='f1_macro',\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Aggregate results\n",
    "mean_f1 = np.mean(cv_results['test_score'])\n",
    "std_f1 = np.std(cv_results['test_score'])\n",
    "train_f1 = np.mean(cv_results['train_score'])\n",
    "\n",
    "# Fit and evaluate\n",
    "pipeline.fit(X_train_var, y_train)\n",
    "y_pred = pipeline.predict(X_test_var)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Detect overfitting\n",
    "overfitting = train_f1 - mean_f1 > 0.1\n",
    "\n",
    "# Append results for comparison\n",
    "results.append({\n",
    "    'Experiment': 'Experiment 4',\n",
    "    'Model': 'Logistic Regression with Feature Selection',\n",
    "    'F1': f1,\n",
    "    'Accuracy': accuracy,\n",
    "    'Overfitting': overfitting\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "print(accuracy)\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the results as a table\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 5: PCA\n",
    "pca = PCA(n_components=5)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('classifier', LogisticRegression(max_iter=2000, random_state=42))\n",
    "])\n",
    "\n",
    "cv_results = cross_validate(pipeline, X_train_pca, y_train, cv=10, scoring='f1_macro', return_train_score=True)\n",
    "\n",
    "mean_f1 = np.mean(cv_results['test_score'])\n",
    "std_f1 = np.std(cv_results['test_score'])\n",
    "train_f1 = np.mean(cv_results['train_score'])\n",
    "\n",
    "pipeline.fit(X_train_pca, y_train)\n",
    "y_pred = pipeline.predict(X_test_pca)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "overfitting = train_f1 - mean_f1 > 0.1  # Check for overfitting\n",
    "\n",
    "# Append results for comparison\n",
    "results.append({'Experiment': 'Experiment 5', 'Model': 'Logistic Regression', 'F1': f1, 'Accuracy': accuracy, 'Overfitting': overfitting})\n",
    "\n",
    "# Save results and compare\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('experiment_results.csv', index=False)\n",
    "\n",
    "# Plot F1-scores for comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(results_df['Experiment'], results_df['F1'], color='skyblue')\n",
    "plt.xlabel('F1 Score')\n",
    "plt.ylabel('Experiment')\n",
    "plt.title('Comparison of Experiments by F1 Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best model\n",
    "results_df['IsBest'] = results_df['Overfitting'] == False\n",
    "best_idx = results_df[results_df['IsBest']]['F1'].idxmax()  # Get the index of the highest F1 score\n",
    "best_experiment = results_df.loc[best_idx]  # Use loc to get the row corresponding to that index\n",
    "print(\"Best Experiment:\", best_experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import sqlite3\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, FunctionTransformer\n",
    "# from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "# from sklearn.feature_selection import VarianceThreshold\n",
    "# from sklearn.metrics import make_scorer, f1_score, precision_score, recall_score, accuracy_score, confusion_matrix\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Connect to the database and load the dataset\n",
    "# conn = sqlite3.connect('mobile_phones.db')\n",
    "# query = \"\"\"\n",
    "# SELECT \n",
    "#     p.phone_id,\n",
    "#     p.battery_power,\n",
    "#     p.clock_speed,\n",
    "#     p.m_dep,\n",
    "#     p.mobile_wt,\n",
    "#     p.n_cores,\n",
    "#     p.ram,\n",
    "#     p.talk_time,\n",
    "#     p.price_range,\n",
    "#     s.px_height,\n",
    "#     s.px_width,\n",
    "#     s.sc_h,\n",
    "#     s.sc_w,\n",
    "#     c.fc as front_camera,\n",
    "#     c.pc as primary_camera,\n",
    "#     f.blue,\n",
    "#     f.dual_sim,\n",
    "#     f.four_g,\n",
    "#     f.touch_screen,\n",
    "#     f.wifi,\n",
    "#     st.int_memory\n",
    "# FROM phones p\n",
    "# JOIN screen_specs s ON p.phone_id = s.phone_id\n",
    "# JOIN camera_specs c ON p.phone_id = c.phone_id\n",
    "# JOIN phone_features f ON p.phone_id = f.phone_id\n",
    "# JOIN storage_specs st ON p.phone_id = st.phone_id\n",
    "# \"\"\"\n",
    "# df_db = pd.read_sql_query(query, conn)\n",
    "# conn.close()\n",
    "\n",
    "# # Preprocessing pipeline\n",
    "# numerical_features = [\n",
    "#     'battery_power', 'clock_speed', 'm_dep', 'mobile_wt',\n",
    "#     'n_cores', 'ram', 'talk_time', 'sc_h', 'sc_w', 'int_memory'\n",
    "# ]\n",
    "# categorical_features = ['blue', 'dual_sim', 'four_g', 'touch_screen', 'wifi']\n",
    "\n",
    "# numerical_pipeline = Pipeline([\n",
    "#     ('scaler', StandardScaler()),\n",
    "#     ('minmax', MinMaxScaler()),\n",
    "#     ('log', FunctionTransformer(np.log1p))\n",
    "# ])\n",
    "\n",
    "# preprocessor = ColumnTransformer([\n",
    "#     ('num', numerical_pipeline, numerical_features),\n",
    "#     ('cat', OneHotEncoder(), categorical_features)\n",
    "# ])\n",
    "\n",
    "# # Define scoring metrics\n",
    "# scoring = {\n",
    "#     'f1_macro': make_scorer(f1_score, average='macro'),\n",
    "#     'precision_macro': make_scorer(precision_score, average='macro'),\n",
    "#     'recall_macro': make_scorer(recall_score, average='macro'),\n",
    "#     'accuracy': make_scorer(accuracy_score)\n",
    "# }\n",
    "\n",
    "# # Split dataset\n",
    "# X = df_db.drop(columns=['price_range', 'phone_id'])\n",
    "# y = df_db['price_range']\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Initialize experiment tracking\n",
    "# results = []\n",
    "\n",
    "# # Experiment 6: Advanced Ensemble Model (Gradient Boosting)\n",
    "# pipeline = Pipeline([\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     ('classifier', GradientBoostingClassifier(random_state=42))\n",
    "# ])\n",
    "\n",
    "# param_grid = {\n",
    "#     'classifier__n_estimators': [100, 200],\n",
    "#     'classifier__learning_rate': [0.01, 0.1, 0.2],\n",
    "#     'classifier__max_depth': [3, 5, 7]\n",
    "# }\n",
    "\n",
    "# grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='f1_macro')\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# best_model = grid_search.best_estimator_\n",
    "# y_pred = best_model.predict(X_test)\n",
    "\n",
    "# f1 = f1_score(y_test, y_pred, average='macro')\n",
    "# precision = precision_score(y_test, y_pred, average='macro')\n",
    "# recall = recall_score(y_test, y_pred, average='macro')\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# results.append({'Experiment': 'Experiment 6', 'Model': 'Gradient Boosting', 'F1': f1, 'Accuracy': accuracy, 'Overfitting': False})\n",
    "\n",
    "# # Experiment 7: Combining Feature Selection and PCA\n",
    "# variance_thresholder = VarianceThreshold(threshold=0.1)\n",
    "# X_train_var = variance_thresholder.fit_transform(X_train)\n",
    "# X_test_var = variance_thresholder.transform(X_test)\n",
    "\n",
    "# pca = PCA(n_components=5)\n",
    "# X_train_pca = pca.fit_transform(X_train_var)\n",
    "# X_test_pca = pca.transform(X_test_var)\n",
    "\n",
    "# pipeline = Pipeline([\n",
    "#     ('classifier', RandomForestClassifier(random_state=42))\n",
    "# ])\n",
    "\n",
    "# pipeline.fit(X_train_pca, y_train)\n",
    "# y_pred = pipeline.predict(X_test_pca)\n",
    "\n",
    "# f1 = f1_score(y_test, y_pred, average='macro')\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# results.append({'Experiment': 'Experiment 7', 'Model': 'Random Forest with PCA', 'F1': f1, 'Accuracy': accuracy, 'Overfitting': False})\n",
    "\n",
    "# # Save results and compare\n",
    "# results_df = pd.DataFrame(results)\n",
    "# results_df.to_csv('experiment_results.csv', index=False)\n",
    "\n",
    "# # Plot F1-scores for comparison\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.barh(results_df['Experiment'], results_df['F1'], color='skyblue')\n",
    "# plt.xlabel('F1 Score')\n",
    "# plt.ylabel('Experiment')\n",
    "# plt.title('Comparison of Experiments by F1 Score')\n",
    "# plt.show()\n",
    "\n",
    "# # Find the best model\n",
    "# results_df['IsBest'] = results_df['Overfitting'] == False\n",
    "# best_experiment = results_df[results_df['IsBest']].loc[results_df['F1'].idxmax()]\n",
    "# print(\"Best Experiment:\", best_experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame to illustrate functionality\n",
    "data = {\n",
    "    \"Experiment\": [\n",
    "        \"Experiment 1\", \"Experiment 2: XGBoost\", \"Experiment 2: Random Forest\",\n",
    "        \"Experiment 2: Ridge Classifier\", \"Experiment 3\", \"Experiment 4\",\n",
    "        \"Experiment 5\", \"Experiment 6\", \"Experiment 7\"\n",
    "    ],\n",
    "    \"F1\": [0.791, 0.909, 0.802, 0.582, 0.874, 0.762, 0.850, 0.887, 0.909],\n",
    "    \"Accuracy\": [0.798, 0.908, 0.802, 0.580, 0.875, 0.760, 0.852, 0.884, 0.907],\n",
    "    \"Overfitting\": [False, True, False, False, False, False, False, False, False]\n",
    "}\n",
    "results_df = pd.DataFrame(data)\n",
    "\n",
    "# Create the plot for F1 scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(results_df['Experiment'], results_df['F1'], color='skyblue', alpha=0.8, label='F1 Score')\n",
    "plt.axhline(y=results_df['F1'].max(), color='red', linestyle='--', linewidth=1, label='Best F1 Score')\n",
    "\n",
    "# Annotate the best model\n",
    "best_experiment = results_df.loc[results_df['F1'].idxmax()]\n",
    "plt.text(\n",
    "    x=results_df['Experiment'].tolist().index(best_experiment['Experiment']),\n",
    "    y=best_experiment['F1'] + 0.01,\n",
    "    s=f\"Best: {best_experiment['Experiment']} ({best_experiment['F1']:.3f})\",\n",
    "    color='red', fontsize=10, ha='center'\n",
    ")\n",
    "\n",
    "# Add plot details\n",
    "plt.title('F1-Score Comparison Across Experiments', fontsize=14)\n",
    "plt.xlabel('Experiments', fontsize=12)\n",
    "plt.ylabel('F1 Score', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although XGBoost was found to be the best model, it had overfitting problem so i went with Random forest instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "\n",
    "try:\n",
    "    # Combine preprocessing, PCA, and classifier into one pipeline\n",
    "    full_pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),  # Preprocessing steps\n",
    "        ('pca', PCA(n_components=5)),    # PCA step\n",
    "        ('classifier', RandomForestClassifier(random_state=42))  # Classifier\n",
    "    ])\n",
    "\n",
    "    # Fit the full pipeline\n",
    "    full_pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Save the full pipeline\n",
    "    model_filename = \"random_forest_pca_full_pipeline.joblib\"\n",
    "    joblib.dump(full_pipeline, model_filename)\n",
    "    print(f\"Full pipeline saved as {model_filename}\")\n",
    "\n",
    "except Exception:\n",
    "    print(\"Failed to run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    # Load the full pipeline\n",
    "    model_filename = \"random_forest_pca_full_pipeline.joblib\"\n",
    "    full_pipeline = joblib.load(model_filename)\n",
    "    print(f\"Full pipeline loaded from {model_filename}\")\n",
    "\n",
    "    # Load test cases\n",
    "    test_samples_file = \"test_samples.json\"\n",
    "    test_cases = pd.read_json(test_samples_file)\n",
    "    print(\"Test cases loaded:\")\n",
    "    print(test_cases)\n",
    "\n",
    "    # Use the full pipeline to predict\n",
    "    predictions = full_pipeline.predict(test_cases)\n",
    "\n",
    "    # Display results\n",
    "    test_cases['Predicted Price Range'] = predictions\n",
    "    print(\"Test case predictions:\")\n",
    "    print(test_cases)\n",
    "\n",
    "    # Save results for further inspection\n",
    "    test_cases.to_csv(\"test_case_predictions.csv\", index=False)\n",
    "    print(\"Predictions saved to test_case_predictions.csv\")\n",
    "\n",
    "except Exception:\n",
    "    print(\"Failed to run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "print(accuracy)\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the results as a table\n",
    "display(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
